#配置端口
server.port=8083

spring.application.name=wukong-scripture

#自定义bean覆盖已有同名bean
spring.main.allow-bean-definition-overriding=true

#有用户名和密码的格式是spring.data.mongodb.uri=mongodb://name:pass@localhost:27017/test，其中name是用户名，pass是密码
spring.data.mongodb.uri=mongodb://121.43.191.104:27017/test

#=============== kafka consumer  =======================
spring.kafka.bootstrap-servers=121.43.191.104:9092
# 指定默认消费者group id --> 由于在kafka中，同一组中的consumer不会读取到同一个消息，依靠groud.id设置组名
spring.kafka.consumer.group-id=testGroup
# smallest和largest才有效，如果smallest重新0开始读取，如果是largest从logfile的offset读取。一般情况下我们都是设置smallest
spring.kafka.consumer.auto-offset-reset=earliest
# enable.auto.commit:true --> 设置自动提交offset
spring.kafka.consumer.enable-auto-commit=false
#如果'enable.auto.commit'为true，则消费者偏移自动提交给Kafka的频率（以毫秒为单位），默认值为5000。
spring.kafka.consumer.auto-commit-interval=100
# 指定消息key和消息体的编解码方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.max-poll-records=50
spring.kafka.consumer.max-poll-interval=5000
#默认是 1 字节，表示只要 Kafka Broker 端积攒了 1 字节的数据，就可以返回给 Consumer 端，这实在是太小了。我们还是让 Broker 端一次性多返回点数据吧。
spring.kafka.consumer.fetch-min-size=16384
#要保证 Consumer 实例在被判定为 “dead” 之前，能够发送至少 3 轮的心跳请求，即 session.timeout.ms(默认为10秒) >= 3 * heartbeat.interval.ms。
spring.kafka.consumer.heartbeat-interval=3000

#ftp
ftp.enabled=true
ftp.host=127.0.0.1
ftp.port=21
ftp.username=admin
ftp.password=admin